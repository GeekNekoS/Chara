services:
  django:
    build:
      context: .
      dockerfile: Dockerfile.django
    container_name: django_app
    ports:
      - "8000:8000"
    environment:
      # Задайте здесь переменные окружения, если нужно
      - DJANGO_ALLOWED_HOSTS=*
      - TF_SERVING_HOST=flask_api  # Имя хоста для обращения к TensorFlow Serving из Django
      - TF_SERVING_PORT=5000
#    depends_on:
#      - flask_api  # Обеспечивает запуск Inference Server перед Django
    command: >
      sh -c "sleep 5 && python manage.py runserver 0.0.0.0:8000"
    # Задержка на 5 секунд
    # python manage.py runserver 127.0.0.1:8000
    volumes:
      - .:/app  # Монтируем текущую директорию в контейнер
    networks:
      - app_network


  flask-api:
    build:
      context: .
      dockerfile: inference_app/Dockerfile
    container_name: flask_api
    ports:
      - "5000:5000"
    environment:
      - TZ=Etc/UTC
    volumes:
      - ./inference_app:/app
      - ./models:/app/models

    restart: always
    networks:
      - app_network

networks:
  app_network:
    driver: bridge